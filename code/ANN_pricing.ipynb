{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac473f1f695641e7bd13b6da97961c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a66f4ed1aeb3472c8f76b4693bd2ee22",
              "IPY_MODEL_17beb6ff77374d55a8d8124db6e32d4a",
              "IPY_MODEL_93ee078190ef4629b8287d0b164690dd"
            ],
            "layout": "IPY_MODEL_7d1360a2711c43a5a02a6cabc6d90724"
          }
        },
        "a66f4ed1aeb3472c8f76b4693bd2ee22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec1bddf26227497d913e22adfe6eb287",
            "placeholder": "​",
            "style": "IPY_MODEL_edc89b64423541b094f55c17d9771d88",
            "value": "Sanity Checking DataLoader 0:   0%"
          }
        },
        "17beb6ff77374d55a8d8124db6e32d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d32320bf2f854804909c42c060c707ad",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f1c7327adc24902bc7e97940ee71c50",
            "value": 0
          }
        },
        "93ee078190ef4629b8287d0b164690dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66e9df68ea9d4686a95557053c93655e",
            "placeholder": "​",
            "style": "IPY_MODEL_a382e08b95874cbbbde9ed0d374e1fe2",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "7d1360a2711c43a5a02a6cabc6d90724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "ec1bddf26227497d913e22adfe6eb287": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edc89b64423541b094f55c17d9771d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d32320bf2f854804909c42c060c707ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f1c7327adc24902bc7e97940ee71c50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66e9df68ea9d4686a95557053c93655e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a382e08b95874cbbbde9ed0d374e1fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWR0NcHXyIlA",
        "outputId": "40f0200f-3926-4e67-fdff-3650965991de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install polars\n",
        "#!pip install skorch\n",
        "#!pip install pytorch_lightning\n",
        "#!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mh6DZoDyFl6",
        "outputId": "efa4f826-96b8-4ab1-acba-a46bf9c83453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting polars\n",
            "  Downloading polars-0.16.12-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from polars) (4.5.0)\n",
            "Installing collected packages: polars\n",
            "Successfully installed polars-0.16.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "_fci1PsglpRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import polars as pl\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from itertools import cycle\n",
        "from typing import Union, List, Optional\n",
        "from sklearn import preprocessing\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import statsmodels.formula.api as smf\n",
        "#import optuna\n",
        "from torch import nn\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "\n",
        "import warnings\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "#import pytorch_lightning as pl\n",
        "#from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "#from skorch import NeuralNetRegressor"
      ],
      "metadata": {
        "id": "_Ss4uDCPyDNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/practicum_data\")\n",
        "from share_funcs import *"
      ],
      "metadata": {
        "id": "2h_yFBV1zELr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/practicum_data-American_Option/eurocall_fourier2.csv\", index_col=0)\n",
        "#data['euro_ame_diff'] = data.g_amercall_fde - data.g_eurocall\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HDMAb5BOKX0G",
        "outputId": "609cf4d8-1f07-4de1-ef02-3c9799dc4198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          m         T         r         q        v0     theta     kappa  \\\n",
              "0  4.258717  1.680247  0.016304  0.041019  0.323511  1.807475  1.571375   \n",
              "1  2.274916  1.804915  0.058924  0.027882  0.496085  1.162404  0.670037   \n",
              "2  4.534160  0.670730  0.020643  0.030725  0.091455  1.815181  0.991059   \n",
              "3  4.753463  0.846062  0.026829  0.042381  0.145451  1.897037  0.809850   \n",
              "4  0.673201  0.114297  0.023764  0.021375  0.271200  0.301441  1.161399   \n",
              "\n",
              "      sigma       rho  eurocall_fourier  \n",
              "0  0.067568 -0.773899          0.191405  \n",
              "1  0.097001 -0.533752          0.226636  \n",
              "2  0.493723 -0.425484          0.001174  \n",
              "3  0.138594 -0.069301          0.007853  \n",
              "4  0.331472 -0.601016          0.327119  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f525c0d-3756-45b4-9fa8-8c231bd15dcb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>m</th>\n",
              "      <th>T</th>\n",
              "      <th>r</th>\n",
              "      <th>q</th>\n",
              "      <th>v0</th>\n",
              "      <th>theta</th>\n",
              "      <th>kappa</th>\n",
              "      <th>sigma</th>\n",
              "      <th>rho</th>\n",
              "      <th>eurocall_fourier</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.258717</td>\n",
              "      <td>1.680247</td>\n",
              "      <td>0.016304</td>\n",
              "      <td>0.041019</td>\n",
              "      <td>0.323511</td>\n",
              "      <td>1.807475</td>\n",
              "      <td>1.571375</td>\n",
              "      <td>0.067568</td>\n",
              "      <td>-0.773899</td>\n",
              "      <td>0.191405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.274916</td>\n",
              "      <td>1.804915</td>\n",
              "      <td>0.058924</td>\n",
              "      <td>0.027882</td>\n",
              "      <td>0.496085</td>\n",
              "      <td>1.162404</td>\n",
              "      <td>0.670037</td>\n",
              "      <td>0.097001</td>\n",
              "      <td>-0.533752</td>\n",
              "      <td>0.226636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.534160</td>\n",
              "      <td>0.670730</td>\n",
              "      <td>0.020643</td>\n",
              "      <td>0.030725</td>\n",
              "      <td>0.091455</td>\n",
              "      <td>1.815181</td>\n",
              "      <td>0.991059</td>\n",
              "      <td>0.493723</td>\n",
              "      <td>-0.425484</td>\n",
              "      <td>0.001174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.753463</td>\n",
              "      <td>0.846062</td>\n",
              "      <td>0.026829</td>\n",
              "      <td>0.042381</td>\n",
              "      <td>0.145451</td>\n",
              "      <td>1.897037</td>\n",
              "      <td>0.809850</td>\n",
              "      <td>0.138594</td>\n",
              "      <td>-0.069301</td>\n",
              "      <td>0.007853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.673201</td>\n",
              "      <td>0.114297</td>\n",
              "      <td>0.023764</td>\n",
              "      <td>0.021375</td>\n",
              "      <td>0.271200</td>\n",
              "      <td>0.301441</td>\n",
              "      <td>1.161399</td>\n",
              "      <td>0.331472</td>\n",
              "      <td>-0.601016</td>\n",
              "      <td>0.327119</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f525c0d-3756-45b4-9fa8-8c231bd15dcb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f525c0d-3756-45b4-9fa8-8c231bd15dcb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f525c0d-3756-45b4-9fa8-8c231bd15dcb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data = pd.read_csv(\"/content/drive/MyDrive/practicum_data-American_Option/generated_data_50k.csv\", index_col=0)\n",
        "#data['g_eurocall'] = data['g_eurocall'].apply(lambda x: 0 if x < 1e-5 else x)\n",
        "#data['euro_ame_diff'] = data.g_amercall_fde - data.g_eurocall\n",
        "#(data.euro_ame_diff > 0).value_counts()\n",
        "\n",
        "xgb_data = copy.deepcopy(data)\n",
        "\n",
        "X_xgb = xgb_data.drop(columns=['eurocall_fourier'], axis=1)\n",
        "#X_xgb = X_xgb.drop(columns=['g_eurocall'], axis=1)\n",
        "y_xgb = xgb_data['eurocall_fourier']\n",
        "\n",
        "#X_xgb['T_sigma'] = X_xgb.g_T * X_xgb.g_sigma\n",
        "#X_xgb['T_q'] = X_xgb.g_T * X_xgb.g_q\n",
        "#X_xgb['T_r'] = X_xgb.g_T * X_xgb.g_r\n",
        "\n",
        "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_xgb, y_xgb, test_size=0.3, random_state=10)\n",
        "train_size = int(0.8 * len(X_train_xgb))\n",
        "X_train_xgb, X_val_xgb, y_train_xgb, y_val_xgb = train_test_split(X_train_xgb, y_train_xgb, test_size=0.3, random_state=10)"
      ],
      "metadata": {
        "id": "wuWMQUgX4RaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7cQO_WLsTNXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_xgb.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FU660br8P28v",
        "outputId": "141e8b48-a11c-4e28-98e3-a3013f8a2b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              m         T         r         q        v0     theta     kappa  \\\n",
              "27632  1.577089  1.510236  0.052815  0.010955  0.234157  1.110906  1.107445   \n",
              "36119  1.607342  1.520096  0.061065  0.021706  0.118032  1.037533  1.989870   \n",
              "4796   4.753064  1.576210  0.073410  0.041019  0.313377  0.842204  0.311313   \n",
              "3648   2.219167  1.591208  0.061564  0.006440  0.073744  1.265429  1.141951   \n",
              "24501  2.342002  1.616950  0.006100  0.013712  0.191482  0.171685  0.556774   \n",
              "\n",
              "          sigma       rho  \n",
              "27632  0.681154 -0.214079  \n",
              "36119  0.226360 -0.712703  \n",
              "4796   0.961561  0.279108  \n",
              "3648   0.096655 -0.339541  \n",
              "24501  0.127366 -0.249947  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0e6f6058-63e2-4f1f-a3e2-4c34bf9fce6f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>m</th>\n",
              "      <th>T</th>\n",
              "      <th>r</th>\n",
              "      <th>q</th>\n",
              "      <th>v0</th>\n",
              "      <th>theta</th>\n",
              "      <th>kappa</th>\n",
              "      <th>sigma</th>\n",
              "      <th>rho</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27632</th>\n",
              "      <td>1.577089</td>\n",
              "      <td>1.510236</td>\n",
              "      <td>0.052815</td>\n",
              "      <td>0.010955</td>\n",
              "      <td>0.234157</td>\n",
              "      <td>1.110906</td>\n",
              "      <td>1.107445</td>\n",
              "      <td>0.681154</td>\n",
              "      <td>-0.214079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36119</th>\n",
              "      <td>1.607342</td>\n",
              "      <td>1.520096</td>\n",
              "      <td>0.061065</td>\n",
              "      <td>0.021706</td>\n",
              "      <td>0.118032</td>\n",
              "      <td>1.037533</td>\n",
              "      <td>1.989870</td>\n",
              "      <td>0.226360</td>\n",
              "      <td>-0.712703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4796</th>\n",
              "      <td>4.753064</td>\n",
              "      <td>1.576210</td>\n",
              "      <td>0.073410</td>\n",
              "      <td>0.041019</td>\n",
              "      <td>0.313377</td>\n",
              "      <td>0.842204</td>\n",
              "      <td>0.311313</td>\n",
              "      <td>0.961561</td>\n",
              "      <td>0.279108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3648</th>\n",
              "      <td>2.219167</td>\n",
              "      <td>1.591208</td>\n",
              "      <td>0.061564</td>\n",
              "      <td>0.006440</td>\n",
              "      <td>0.073744</td>\n",
              "      <td>1.265429</td>\n",
              "      <td>1.141951</td>\n",
              "      <td>0.096655</td>\n",
              "      <td>-0.339541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24501</th>\n",
              "      <td>2.342002</td>\n",
              "      <td>1.616950</td>\n",
              "      <td>0.006100</td>\n",
              "      <td>0.013712</td>\n",
              "      <td>0.191482</td>\n",
              "      <td>0.171685</td>\n",
              "      <td>0.556774</td>\n",
              "      <td>0.127366</td>\n",
              "      <td>-0.249947</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e6f6058-63e2-4f1f-a3e2-4c34bf9fce6f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e6f6058-63e2-4f1f-a3e2-4c34bf9fce6f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e6f6058-63e2-4f1f-a3e2-4c34bf9fce6f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6OGShS1eAPv",
        "outputId": "f7a8426a-23fb-4dc4-a622-512e79b520b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTGfhCy9x2mp"
      },
      "outputs": [],
      "source": [
        "class PricingDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X: pd.DataFrame, y:Union[pd.DataFrame, pd.Series]):\n",
        "    \"\"\"Initializes instance of class StudentsPerformanceDataset.\n",
        "    Args:\n",
        "        csv_file (str): Path to the csv file with the students data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Save target and predictors\n",
        "    self.X = X.values.astype(np.float32)\n",
        "    self.y = y.values.reshape(-1,1).astype(np.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      # Convert idx from tensor to list due to pandas bug (that arises when using pytorch's random_split)\n",
        "    #if isinstance(idx, torch.Tensor):\n",
        "      #idx = idx.tolist()\n",
        "\n",
        "    return self.X[idx], self.y[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PricingNetwork(pl.LightningModule):\n",
        "  def __init__(self, model, mean=X_train_xgb.mean(axis=0).values.astype(np.float32), std=X_train_xgb.std(axis=0).values.astype(np.float32), x_features=X_train_xgb.shape[1]):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = (x - self.mean) / self.std\n",
        "    return self.model(x)\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    # training_step defines the train loop.\n",
        "    x, y = batch\n",
        "    y_pred = self(x)\n",
        "    loss = F.mse_loss(y_pred, y)\n",
        "    self.log(\"train_loss\", loss, on_epoch=True)\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "    return optimizer\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    # this is the test loop\n",
        "    x, y = batch\n",
        "    y_pred = self(x)\n",
        "    test_loss = F.mse_loss(y_pred, y)\n",
        "    #self.log(\"test_loss_quantile\", pd.)\n",
        "    self.log(\"test_loss\", test_loss, on_epoch=True)\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    y_pred = self(x)\n",
        "    loss = F.mse_loss(y_pred, y)\n",
        "    # Calling self.log will surface up scalars for you in TensorBoard\n",
        "    self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
        "\n",
        "    return self(batch)"
      ],
      "metadata": {
        "id": "ubvdKXkDiG72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torch.utils.data.DataLoader(PricingDataset(X_train_xgb, y_train_xgb), batch_size=512)\n",
        "val_dataset = torch.utils.data.DataLoader(PricingDataset(X_val_xgb, y_val_xgb), batch_size=512)\n",
        "test_dataset = torch.utils.data.DataLoader(PricingDataset(X_test_xgb, y_test_xgb), batch_size=512)"
      ],
      "metadata": {
        "id": "otvQoH8Brsdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pricing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhsPbPIFzSAu",
        "outputId": "9b31dc0a-581a-4856-b91c-8f12dd05e07a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PricingNetwork(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.01)\n",
            "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (5): LeakyReLU(negative_slope=0.01)\n",
            "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (7): LeakyReLU(negative_slope=0.01)\n",
            "    (8): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (9): LeakyReLU(negative_slope=0.01)\n",
            "    (10): Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neurons = 64\n",
        "x_features = X_train_xgb.shape[1]\n",
        "seq = nn.Sequential(nn.Linear(x_features, neurons), nn.LeakyReLU(),\n",
        "                                     nn.Linear(neurons, neurons), nn.LeakyReLU(),\n",
        "                                     nn.Linear(neurons, neurons), nn.LeakyReLU(),\n",
        "                                     nn.Linear(neurons, neurons), nn.LeakyReLU(),\n",
        "                                     #nn.Linear(neurons, neurons), nn.LeakyReLU(),\n",
        "                                     nn.Linear(neurons, neurons), nn.LeakyReLU(),\n",
        "                                     nn.Linear(neurons, 1))\n",
        "pricing = PricingNetwork(seq)\n",
        "trainer = pl.Trainer(max_epochs=5)\n",
        "trainer.fit(model=pricing, train_dataloaders=train_dataset, val_dataloaders = val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661,
          "referenced_widgets": [
            "ac473f1f695641e7bd13b6da97961c1a",
            "a66f4ed1aeb3472c8f76b4693bd2ee22",
            "17beb6ff77374d55a8d8124db6e32d4a",
            "93ee078190ef4629b8287d0b164690dd",
            "7d1360a2711c43a5a02a6cabc6d90724",
            "ec1bddf26227497d913e22adfe6eb287",
            "edc89b64423541b094f55c17d9771d88",
            "d32320bf2f854804909c42c060c707ad",
            "1f1c7327adc24902bc7e97940ee71c50",
            "66e9df68ea9d4686a95557053c93655e",
            "a382e08b95874cbbbde9ed0d374e1fe2"
          ]
        },
        "id": "ZH-H6pNzi7Ub",
        "outputId": "cfc970d1-c77d-4d3c-a97d-c668ac8d0146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type       | Params\n",
            "-------------------------------------\n",
            "0 | model | Sequential | 17.3 K\n",
            "-------------------------------------\n",
            "17.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "17.3 K    Total params\n",
            "0.069     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac473f1f695641e7bd13b6da97961c1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-901e026222d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpricing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPricingNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpricing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_unwrap_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    609\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;31m# enable train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m                 \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_dataloaders\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataloader_idx\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mdl_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_max_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# store batch level output per dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \"\"\"\n\u001b[1;32m    233\u001b[0m         \u001b[0mhook_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test_step\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{self.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidationStep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-0bcbff704206>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-0bcbff704206>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (8,) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loop(test_dataset, pricing, F.mse_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7l8hbC4xAIt",
        "outputId": "236ab04a-ccc9-467a-ac4c-8540e4b990f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss: 0.677896 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def skorch_train():\n",
        "  net = NeuralNetRegressor(\n",
        "    PricingNetwork,\n",
        "    optimizer=torch.optim.Adam,\n",
        "    max_epochs=30,\n",
        "    batch_size=512,\n",
        "    criterion=nn.MSELoss(),\n",
        "    #lr=0.01,\n",
        "    # Shuffle training data on each epoch\n",
        "    iterator_train__shuffle=True,\n",
        "  )\n",
        "  X_train_ann = X_train_xgb.values.astype(np.float32)\n",
        "  y_train_ann = y_train_xgb.values.reshape(-1,1).astype(np.float32)\n",
        "  net.fit(X_train_ann,y_train_ann)\n",
        ""
      ],
      "metadata": {
        "id": "T5Wqd7rNx8KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skorch_train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK-IOqAkx_ah",
        "outputId": "15c02a58-4a52-4e8b-b197-bacdf780952f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    valid_loss     dur\n",
            "-------  ------------  ------------  ------\n",
            "      1        \u001b[36m0.0032\u001b[0m        \u001b[32m0.0001\u001b[0m  2.3570\n",
            "      2        \u001b[36m0.0001\u001b[0m        \u001b[32m0.0000\u001b[0m  2.3111\n",
            "      3        \u001b[36m0.0001\u001b[0m        \u001b[32m0.0000\u001b[0m  2.3092\n",
            "      4        0.0001        \u001b[32m0.0000\u001b[0m  2.8775\n",
            "      5        \u001b[36m0.0000\u001b[0m        \u001b[32m0.0000\u001b[0m  3.7994\n",
            "      6        0.0001        0.0000  2.3302\n",
            "      7        0.0000        0.0000  2.2888\n",
            "      8        0.0000        0.0000  2.3368\n",
            "      9        0.0000        0.0000  2.3038\n",
            "     10        0.0001        \u001b[32m0.0000\u001b[0m  3.5412\n",
            "     11        0.0000        0.0000  3.1187\n",
            "     12        0.0000        0.0000  2.3303\n",
            "     13        0.0001        0.0000  2.3498\n",
            "     14        \u001b[36m0.0000\u001b[0m        0.0000  2.3437\n",
            "     15        0.0000        0.0001  2.7512\n",
            "     16        0.0001        0.0000  3.9133\n",
            "     17        0.0000        0.0000  2.3941\n",
            "     18        0.0000        \u001b[32m0.0000\u001b[0m  2.3531\n",
            "     19        0.0000        0.0000  2.3629\n",
            "     20        0.0001        0.0000  2.3588\n",
            "     21        \u001b[36m0.0000\u001b[0m        0.0000  3.5367\n",
            "     22        \u001b[36m0.0000\u001b[0m        0.0000  3.1627\n",
            "     23        0.0000        0.0000  2.3704\n",
            "     24        0.0000        0.0000  2.4275\n",
            "     25        0.0000        0.0000  2.4000\n",
            "     26        0.0000        0.0000  2.8408\n",
            "     27        0.0000        0.0000  3.8766\n",
            "     28        0.0000        0.0000  2.3642\n",
            "     29        0.0000        0.0000  2.3301\n",
            "     30        0.0000        \u001b[32m0.0000\u001b[0m  2.3661\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
              "  module_=PricingNetwork2(\n",
              "    (bottleneck): ModuleList(\n",
              "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (3): ReLU()\n",
              "      (4): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (5): ReLU()\n",
              "      (6): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (7): ReLU()\n",
              "      (8): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (9): ReLU()\n",
              "    )\n",
              "    (final_layer): Linear(in_features=64, out_features=1, bias=True)\n",
              "  ),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PricingNetwork2(nn.Module):\n",
        "  def __init__(self, mean=X_train_xgb.mean(axis=0).values.astype(np.float32), std=X_train_xgb.std(axis=0).values.astype(np.float32), neurons=64, x_features=X_train_xgb.shape[1]):\n",
        "  #def __init__(self, neurons=64, x_features=5):\n",
        "    super(PricingNetwork2,self).__init__()\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "    self.bottleneck = nn.Sequential(nn.Linear(x_features, neurons), nn.LeakyReLU(),\n",
        "                                     nn.Linear(neurons, neurons), nn.LeakyReLU(),\n",
        "                                     nn.Linear(neurons, neurons), nn.LeakyReLU(),\n",
        "                                     nn.Linear(neurons, neurons), nn.LeakyReLU(),\n",
        "                                     #nn.Linear(neurons, neurons), nn.LeakyReLU(),\n",
        "                                     nn.Linear(neurons, neurons), nn.LeakyReLU())\n",
        "    #self.bottleneck = nn.ModuleList([nn.Linear(x_features, neurons), nn.LeakyReLU()] + [nn.Linear(neurons, neurons), nn.LeakyReLU()] * 4)\n",
        "    self.final_layer = nn.Linear(neurons, 1)\n",
        "\n",
        "  def forward(self, X, **kwargs):\n",
        "    #X = (X - np.array([2.59564865,1.00528802,0.03986204,0.02523768,1.06051767], dtype=np.float32)) / np.array([1.38237971, 0.57541136, 0.02325452, 0.01445998, 0.55268058],dtype=np.float32)\n",
        "    #(X - X.mean(axis=0))/X.std(axis=0)\n",
        "    X = (X - self.mean) / self.std\n",
        "    X = self.bottleneck(X)\n",
        "    X  = self.final_layer(X)\n",
        "    return X"
      ],
      "metadata": {
        "id": "QJEy-zNETf0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "#    train_loss = 0.\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "#        train_loss += loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "#    return train_loss/num_batches\n",
        "\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "\n",
        "    test_loss = test_loss/num_batches\n",
        "    return test_loss\n"
      ],
      "metadata": {
        "id": "dTvBUR8M1VOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PricingNetwork2(x_features=X_train_xgb.shape[1],mean=X_train_xgb.mean(axis=0).values.astype(np.float32), std=X_train_xgb.std(axis=0).values.astype(np.float32))\n",
        "#loss_fn = nn.MSELoss()\n",
        "loss_fn = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
      ],
      "metadata": {
        "id": "dFtJXf0D2nMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 80\n",
        "#train_losses = np.empty(epochs)\n",
        "test_losses = np.empty(epochs)\n",
        "val_losses = np.empty(epochs)\n",
        "train_dataset = torch.utils.data.DataLoader(PricingDataset(X_train_xgb, y_train_xgb), batch_size=128)\n",
        "val_dataset = torch.utils.data.DataLoader(PricingDataset(X_val_xgb, y_val_xgb), batch_size=128)\n",
        "test_dataset = torch.utils.data.DataLoader(PricingDataset(X_test_xgb, y_test_xgb), batch_size=512)\n",
        "for t in range(epochs):\n",
        "    train_loop(train_dataset, model, loss_fn, optimizer)\n",
        "    #train_losses[t] = train_loop(train_dataset, model, loss_fn, optimizer)\n",
        "    val_loss = test_loop(val_dataset, model, loss_fn)\n",
        "    test_loss = test_loop(test_dataset, model, loss_fn)\n",
        "    val_losses[t] = val_loss\n",
        "    test_losses[t] = test_loss\n",
        "    #scheduler.step()\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if t > 25:\n",
        "      print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "      print(f\"Avg loss: {val_loss:>8f} \\n\")\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuVu7xLtR5Qy",
        "outputId": "531532b4-f5ac-4aa0-b8f9-08a277b280fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27\n",
            "-------------------------------\n",
            "Avg loss: 0.003412 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Avg loss: 0.004373 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Avg loss: 0.003434 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Avg loss: 0.003815 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Avg loss: 0.003761 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Avg loss: 0.003228 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Avg loss: 0.003268 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Avg loss: 0.006025 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Avg loss: 0.003340 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Avg loss: 0.004839 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Avg loss: 0.004241 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Avg loss: 0.003175 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Avg loss: 0.003495 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Avg loss: 0.003243 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Avg loss: 0.004564 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Avg loss: 0.003480 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Avg loss: 0.002963 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Avg loss: 0.003330 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Avg loss: 0.003279 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Avg loss: 0.002887 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Avg loss: 0.003074 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Avg loss: 0.004273 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Avg loss: 0.003221 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Avg loss: 0.002708 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Avg loss: 0.004439 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Avg loss: 0.002834 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Avg loss: 0.002938 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Avg loss: 0.002891 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Avg loss: 0.002689 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Avg loss: 0.003741 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Avg loss: 0.002728 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Avg loss: 0.002872 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Avg loss: 0.003556 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Avg loss: 0.003561 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Avg loss: 0.003276 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Avg loss: 0.003353 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Avg loss: 0.003464 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Avg loss: 0.005324 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Avg loss: 0.002958 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Avg loss: 0.002532 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Avg loss: 0.002505 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Avg loss: 0.002391 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Avg loss: 0.006129 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Avg loss: 0.002625 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Avg loss: 0.002519 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Avg loss: 0.003791 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Avg loss: 0.002734 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Avg loss: 0.002589 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Avg loss: 0.004503 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Avg loss: 0.002662 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Avg loss: 0.002827 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Avg loss: 0.002490 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Avg loss: 0.002906 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Avg loss: 0.001703 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(val_losses, label=\"val set\")\n",
        "plt.plot(test_losses, label='test set');\n",
        "plt.title(\"\")\n",
        "plt.ylabel(\"MAE\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "2xmj3TZxWs2V",
        "outputId": "cd4c9831-9634-4383-d906-027bee8a7fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f8311d487c0>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('ANN Absolute Error (quantile): ')\n",
        "val_dataset = torch.utils.data.DataLoader(PricingDataset(X_val_xgb, y_val_xgb), batch_size=128)\n",
        "with torch.no_grad():\n",
        "   print((y_val_xgb - pd.Series(model(torch.tensor(X_val_xgb.values.astype(np.float32))).flatten(), index=X_val_xgb.index)).abs().quantile([0,0.25,0.5,0.75,0.85,0.9,0.95, 0.99,1]).round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW_BnqW4ZWxT",
        "outputId": "fee139b3-edb4-4b7f-dcbb-089ca4301d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN Absolute Error (quantile): \n",
            "0.00    0.000\n",
            "0.25    0.000\n",
            "0.50    0.001\n",
            "0.75    0.002\n",
            "0.85    0.003\n",
            "0.90    0.004\n",
            "0.95    0.005\n",
            "0.99    0.009\n",
            "1.00    0.650\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  predict_ = pd.Series(model(torch.tensor(X_test_xgb.values.astype(np.float32))).flatten(), index=X_test_xgb.index)\n",
        "predict_[predict_ < 0] = 0."
      ],
      "metadata": {
        "id": "s_ytFMTj7Qt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ind_itm = X_test_xgb.m < 1\n",
        "print((y_test_xgb[ind_itm] - predict_[ind_itm]).abs().quantile([0,0.25,0.5,0.75,0.85,0.9,0.95, 0.99,1]).round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eh4kP7u8hwb",
        "outputId": "ece9dbb2-1b58-40a7-c8e4-e41e061b385c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00    0.000\n",
            "0.25    0.001\n",
            "0.50    0.002\n",
            "0.75    0.004\n",
            "0.85    0.006\n",
            "0.90    0.006\n",
            "0.95    0.008\n",
            "0.99    0.013\n",
            "1.00    0.701\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(y_test_xgb[ind_itm] - predict_[ind_itm]).abs().sort_values(ascending=False).head(50).index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygv2L1vW-frw",
        "outputId": "a1ac2a87-67d4-47a8-8efd-7dc56a7e258d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([14677,  9870, 45809,  2333, 39685, 22626, 10746, 39375, 32209,\n",
              "            16589, 43384, 38188, 20297,  2353, 41986, 43911,  4421,  6864,\n",
              "            31306,  6692, 20391,  5792, 37624, 40219, 32313, 39841, 17795,\n",
              "            22245, 28648, 28416, 49556,  2637, 36916, 10448, 46717,  3756,\n",
              "            22927, 32263, 13739,  8470, 18265, 44073, 19267, 28797, 14561,\n",
              "            42161, 38359, 29663, 10023, 39949],\n",
              "           dtype='int64')"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_xgb.loc[[9870]]"
      ],
      "metadata": {
        "id": "tQm7ML15Y0h3",
        "outputId": "573c5e6c-5517-494e-f694-cfedf7ffb860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             m         T         r         q        v0     theta     kappa  \\\n",
              "9870  0.842425  0.001916  0.045908  0.044421  0.234685  1.334382  0.477893   \n",
              "\n",
              "         sigma       rho  \n",
              "9870  0.432607  0.453099  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4ffcace-8ae1-428e-b2e4-3df0b3199ba1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>m</th>\n",
              "      <th>T</th>\n",
              "      <th>r</th>\n",
              "      <th>q</th>\n",
              "      <th>v0</th>\n",
              "      <th>theta</th>\n",
              "      <th>kappa</th>\n",
              "      <th>sigma</th>\n",
              "      <th>rho</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9870</th>\n",
              "      <td>0.842425</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.045908</td>\n",
              "      <td>0.044421</td>\n",
              "      <td>0.234685</td>\n",
              "      <td>1.334382</td>\n",
              "      <td>0.477893</td>\n",
              "      <td>0.432607</td>\n",
              "      <td>0.453099</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4ffcace-8ae1-428e-b2e4-3df0b3199ba1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a4ffcace-8ae1-428e-b2e4-3df0b3199ba1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a4ffcace-8ae1-428e-b2e4-3df0b3199ba1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_xgb.loc[[14677,  9870, 45809,  2333, 39685, 22626, 10746, 39375, 32209,\n",
        "            16589, 43384, 38188, 20297,  2353, 41986, 43911,  4421,  6864,\n",
        "            31306,  6692, 20391,  5792, 37624, 40219, 32313, 39841, 17795,\n",
        "            22245, 28648, 28416, 49556,  2637, 36916, 10448, 46717,  3756,\n",
        "            22927, 32263, 13739,  8470, 18265, 44073, 19267, 28797, 14561,\n",
        "            42161, 38359, 29663, 10023, 39949],:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bBRLTCCC-ql2",
        "outputId": "426b165f-66b3-4d8d-b313-80cd6df52b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              m         T         r         q        v0     theta     kappa  \\\n",
              "14677  0.306817  0.001916  0.058351  0.024609  0.145724  1.873265  1.834680   \n",
              "9870   0.842425  0.001916  0.045908  0.044421  0.234685  1.334382  0.477893   \n",
              "45809  0.590916  1.842050  0.073313  0.022273  0.098367  1.399618  0.068381   \n",
              "2333   0.869552  0.051926  0.079534  0.040447  0.024720  0.132549  0.499828   \n",
              "39685  0.580876  1.891920  0.038813  0.018660  0.364499  0.421284  0.192542   \n",
              "22626  0.229517  1.964997  0.057233  0.002600  0.402269  0.899304  0.122432   \n",
              "10746  0.986658  1.420549  0.037066  0.023402  0.103711  0.042812  0.025919   \n",
              "39375  0.767655  1.977395  0.003649  0.033063  0.218058  0.476168  0.112508   \n",
              "32209  0.895364  0.883997  0.020326  0.014143  0.042890  0.019095  1.396961   \n",
              "16589  0.219077  1.389378  0.019148  0.029049  0.377340  0.068075  0.477467   \n",
              "43384  0.973726  0.029280  0.032988  0.004893  0.436969  1.754797  0.558223   \n",
              "38188  0.874592  1.191526  0.027426  0.002425  0.102502  0.137589  0.725734   \n",
              "20297  0.589624  1.353482  0.003430  0.037286  0.170089  0.100457  0.444150   \n",
              "2353   0.635350  1.423722  0.009961  0.044167  0.110129  1.621712  0.079904   \n",
              "41986  0.989600  0.225088  0.029315  0.037824  0.016684  0.514414  0.044704   \n",
              "43911  0.281758  1.300949  0.020972  0.006706  0.468861  1.608321  0.139157   \n",
              "4421   0.541190  1.621064  0.076300  0.034545  0.018188  0.131850  0.269817   \n",
              "6864   0.842425  0.098523  0.057233  0.039526  0.068432  0.995092  0.301984   \n",
              "31306  0.678185  1.660958  0.010668  0.009599  0.416944  0.025208  0.149201   \n",
              "6692   0.201971  1.103201  0.077293  0.035368  0.498226  0.150282  0.151656   \n",
              "20391  0.229658  1.995146  0.043844  0.003458  0.253876  0.209121  1.693763   \n",
              "5792   0.530683  1.857228  0.000139  0.010801  0.380150  0.032467  0.961106   \n",
              "37624  0.334642  1.543400  0.053172  0.009020  0.229634  0.365733  0.150038   \n",
              "40219  0.440275  1.335393  0.013469  0.000671  0.023475  0.815096  1.518595   \n",
              "32313  0.989600  0.129331  0.023516  0.005730  0.021761  1.122677  0.069091   \n",
              "39841  0.877417  0.062786  0.000058  0.048548  0.248605  1.537453  1.610150   \n",
              "17795  0.701256  0.100493  0.058984  0.039813  0.035690  1.079102  1.126777   \n",
              "22245  0.261364  0.250220  0.007462  0.047443  0.013714  0.342149  1.156354   \n",
              "28648  0.874592  0.008672  0.010865  0.021222  0.455552  0.694042  0.311915   \n",
              "28416  0.391426  1.638694  0.047504  0.024609  0.467344  0.141297  1.709419   \n",
              "49556  0.224927  1.210898  0.051646  0.010157  0.265490  0.371841  0.166089   \n",
              "2637   0.205464  1.335393  0.001645  0.049218  0.272231  0.621764  1.705245   \n",
              "36916  0.338553  1.914491  0.058058  0.046857  0.399522  0.381267  0.260197   \n",
              "10448  0.578737  1.502653  0.045046  0.000284  0.102967  0.340299  0.027642   \n",
              "46717  0.802334  1.578650  0.072476  0.019320  0.044812  0.031356  1.982347   \n",
              "3756   0.201971  1.163303  0.012286  0.019552  0.137493  0.382032  0.676236   \n",
              "22927  0.989600  1.313777  0.079012  0.037436  0.109660  0.572237  0.372417   \n",
              "32263  0.373799  1.409662  0.023764  0.026877  0.023927  0.830039  0.191213   \n",
              "13739  0.590916  0.846197  0.053506  0.009998  0.481487  0.046850  1.585167   \n",
              "8470   0.391528  1.947944  0.017481  0.006486  0.209309  0.667397  0.065715   \n",
              "18265  0.277570  0.489535  0.067724  0.016751  0.493453  1.108177  0.523977   \n",
              "44073  0.810456  1.613509  0.079026  0.047443  0.458519  1.985412  0.703424   \n",
              "19267  0.902621  0.172176  0.018784  0.041871  0.064603  1.685501  0.375954   \n",
              "28797  0.305722  1.672154  0.043157  0.021138  0.263004  1.586433  0.254102   \n",
              "14561  0.703561  0.072972  0.068550  0.000591  0.248605  1.532844  1.846427   \n",
              "42161  0.360970  0.648508  0.013586  0.041877  0.028298  0.833892  0.572515   \n",
              "38359  0.338553  0.228158  0.025037  0.029819  0.069665  0.350182  0.558028   \n",
              "29663  0.261364  1.073403  0.060569  0.013604  0.283790  0.250544  1.759530   \n",
              "10023  0.205464  1.313681  0.013451  0.002058  0.493146  0.602416  1.822098   \n",
              "39949  0.232278  0.055581  0.026690  0.047569  0.432851  1.010763  1.529575   \n",
              "\n",
              "          sigma       rho  \n",
              "14677  0.821470  0.122932  \n",
              "9870   0.432607  0.453099  \n",
              "45809  0.949760  0.474702  \n",
              "2333   0.980860 -0.879862  \n",
              "39685  0.782965  0.890827  \n",
              "22626  0.997124  0.363496  \n",
              "10746  0.243253 -0.434397  \n",
              "39375  0.250753  0.595622  \n",
              "32209  0.759487  0.678637  \n",
              "16589  0.252281  0.086789  \n",
              "43384  0.376196  0.726545  \n",
              "38188  0.689463  0.784522  \n",
              "20297  0.684674  0.772048  \n",
              "2353   0.849330  0.884900  \n",
              "41986  0.533754 -0.279644  \n",
              "43911  0.465332  0.560683  \n",
              "4421   0.954863  0.065250  \n",
              "6864   0.866664 -0.830235  \n",
              "31306  0.653682 -0.242065  \n",
              "6692   0.311190  0.779649  \n",
              "20391  0.145876 -0.365285  \n",
              "5792   0.301722  0.374925  \n",
              "37624  0.485196  0.353832  \n",
              "40219  0.469386 -0.036901  \n",
              "32313  0.699078 -0.667638  \n",
              "39841  0.870066 -0.259337  \n",
              "17795  0.924938 -0.839169  \n",
              "22245  0.451590  0.423300  \n",
              "28648  0.037159  0.683308  \n",
              "28416  0.734972  0.466201  \n",
              "49556  0.350353  0.512853  \n",
              "2637   0.575100 -0.553990  \n",
              "36916  0.822677 -0.581013  \n",
              "10448  0.168833 -0.499132  \n",
              "46717  0.154012 -0.178022  \n",
              "3756   0.489580  0.502518  \n",
              "22927  0.208611 -0.861987  \n",
              "32263  0.724376 -0.596645  \n",
              "13739  0.812196 -0.869474  \n",
              "8470   0.876935 -0.112921  \n",
              "18265  0.058664 -0.881303  \n",
              "44073  0.219107  0.222075  \n",
              "19267  0.770560 -0.469839  \n",
              "28797  0.958788  0.213872  \n",
              "14561  0.271077  0.867225  \n",
              "42161  0.537903 -0.875486  \n",
              "38359  0.954863  0.459732  \n",
              "29663  0.183378 -0.893004  \n",
              "10023  0.079310 -0.366569  \n",
              "39949  0.290168 -0.293424  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2fb27c5-632a-4b41-b545-60483e8cccef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>m</th>\n",
              "      <th>T</th>\n",
              "      <th>r</th>\n",
              "      <th>q</th>\n",
              "      <th>v0</th>\n",
              "      <th>theta</th>\n",
              "      <th>kappa</th>\n",
              "      <th>sigma</th>\n",
              "      <th>rho</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14677</th>\n",
              "      <td>0.306817</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.058351</td>\n",
              "      <td>0.024609</td>\n",
              "      <td>0.145724</td>\n",
              "      <td>1.873265</td>\n",
              "      <td>1.834680</td>\n",
              "      <td>0.821470</td>\n",
              "      <td>0.122932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9870</th>\n",
              "      <td>0.842425</td>\n",
              "      <td>0.001916</td>\n",
              "      <td>0.045908</td>\n",
              "      <td>0.044421</td>\n",
              "      <td>0.234685</td>\n",
              "      <td>1.334382</td>\n",
              "      <td>0.477893</td>\n",
              "      <td>0.432607</td>\n",
              "      <td>0.453099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45809</th>\n",
              "      <td>0.590916</td>\n",
              "      <td>1.842050</td>\n",
              "      <td>0.073313</td>\n",
              "      <td>0.022273</td>\n",
              "      <td>0.098367</td>\n",
              "      <td>1.399618</td>\n",
              "      <td>0.068381</td>\n",
              "      <td>0.949760</td>\n",
              "      <td>0.474702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333</th>\n",
              "      <td>0.869552</td>\n",
              "      <td>0.051926</td>\n",
              "      <td>0.079534</td>\n",
              "      <td>0.040447</td>\n",
              "      <td>0.024720</td>\n",
              "      <td>0.132549</td>\n",
              "      <td>0.499828</td>\n",
              "      <td>0.980860</td>\n",
              "      <td>-0.879862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39685</th>\n",
              "      <td>0.580876</td>\n",
              "      <td>1.891920</td>\n",
              "      <td>0.038813</td>\n",
              "      <td>0.018660</td>\n",
              "      <td>0.364499</td>\n",
              "      <td>0.421284</td>\n",
              "      <td>0.192542</td>\n",
              "      <td>0.782965</td>\n",
              "      <td>0.890827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22626</th>\n",
              "      <td>0.229517</td>\n",
              "      <td>1.964997</td>\n",
              "      <td>0.057233</td>\n",
              "      <td>0.002600</td>\n",
              "      <td>0.402269</td>\n",
              "      <td>0.899304</td>\n",
              "      <td>0.122432</td>\n",
              "      <td>0.997124</td>\n",
              "      <td>0.363496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10746</th>\n",
              "      <td>0.986658</td>\n",
              "      <td>1.420549</td>\n",
              "      <td>0.037066</td>\n",
              "      <td>0.023402</td>\n",
              "      <td>0.103711</td>\n",
              "      <td>0.042812</td>\n",
              "      <td>0.025919</td>\n",
              "      <td>0.243253</td>\n",
              "      <td>-0.434397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39375</th>\n",
              "      <td>0.767655</td>\n",
              "      <td>1.977395</td>\n",
              "      <td>0.003649</td>\n",
              "      <td>0.033063</td>\n",
              "      <td>0.218058</td>\n",
              "      <td>0.476168</td>\n",
              "      <td>0.112508</td>\n",
              "      <td>0.250753</td>\n",
              "      <td>0.595622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32209</th>\n",
              "      <td>0.895364</td>\n",
              "      <td>0.883997</td>\n",
              "      <td>0.020326</td>\n",
              "      <td>0.014143</td>\n",
              "      <td>0.042890</td>\n",
              "      <td>0.019095</td>\n",
              "      <td>1.396961</td>\n",
              "      <td>0.759487</td>\n",
              "      <td>0.678637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16589</th>\n",
              "      <td>0.219077</td>\n",
              "      <td>1.389378</td>\n",
              "      <td>0.019148</td>\n",
              "      <td>0.029049</td>\n",
              "      <td>0.377340</td>\n",
              "      <td>0.068075</td>\n",
              "      <td>0.477467</td>\n",
              "      <td>0.252281</td>\n",
              "      <td>0.086789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43384</th>\n",
              "      <td>0.973726</td>\n",
              "      <td>0.029280</td>\n",
              "      <td>0.032988</td>\n",
              "      <td>0.004893</td>\n",
              "      <td>0.436969</td>\n",
              "      <td>1.754797</td>\n",
              "      <td>0.558223</td>\n",
              "      <td>0.376196</td>\n",
              "      <td>0.726545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38188</th>\n",
              "      <td>0.874592</td>\n",
              "      <td>1.191526</td>\n",
              "      <td>0.027426</td>\n",
              "      <td>0.002425</td>\n",
              "      <td>0.102502</td>\n",
              "      <td>0.137589</td>\n",
              "      <td>0.725734</td>\n",
              "      <td>0.689463</td>\n",
              "      <td>0.784522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20297</th>\n",
              "      <td>0.589624</td>\n",
              "      <td>1.353482</td>\n",
              "      <td>0.003430</td>\n",
              "      <td>0.037286</td>\n",
              "      <td>0.170089</td>\n",
              "      <td>0.100457</td>\n",
              "      <td>0.444150</td>\n",
              "      <td>0.684674</td>\n",
              "      <td>0.772048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2353</th>\n",
              "      <td>0.635350</td>\n",
              "      <td>1.423722</td>\n",
              "      <td>0.009961</td>\n",
              "      <td>0.044167</td>\n",
              "      <td>0.110129</td>\n",
              "      <td>1.621712</td>\n",
              "      <td>0.079904</td>\n",
              "      <td>0.849330</td>\n",
              "      <td>0.884900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41986</th>\n",
              "      <td>0.989600</td>\n",
              "      <td>0.225088</td>\n",
              "      <td>0.029315</td>\n",
              "      <td>0.037824</td>\n",
              "      <td>0.016684</td>\n",
              "      <td>0.514414</td>\n",
              "      <td>0.044704</td>\n",
              "      <td>0.533754</td>\n",
              "      <td>-0.279644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43911</th>\n",
              "      <td>0.281758</td>\n",
              "      <td>1.300949</td>\n",
              "      <td>0.020972</td>\n",
              "      <td>0.006706</td>\n",
              "      <td>0.468861</td>\n",
              "      <td>1.608321</td>\n",
              "      <td>0.139157</td>\n",
              "      <td>0.465332</td>\n",
              "      <td>0.560683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4421</th>\n",
              "      <td>0.541190</td>\n",
              "      <td>1.621064</td>\n",
              "      <td>0.076300</td>\n",
              "      <td>0.034545</td>\n",
              "      <td>0.018188</td>\n",
              "      <td>0.131850</td>\n",
              "      <td>0.269817</td>\n",
              "      <td>0.954863</td>\n",
              "      <td>0.065250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6864</th>\n",
              "      <td>0.842425</td>\n",
              "      <td>0.098523</td>\n",
              "      <td>0.057233</td>\n",
              "      <td>0.039526</td>\n",
              "      <td>0.068432</td>\n",
              "      <td>0.995092</td>\n",
              "      <td>0.301984</td>\n",
              "      <td>0.866664</td>\n",
              "      <td>-0.830235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31306</th>\n",
              "      <td>0.678185</td>\n",
              "      <td>1.660958</td>\n",
              "      <td>0.010668</td>\n",
              "      <td>0.009599</td>\n",
              "      <td>0.416944</td>\n",
              "      <td>0.025208</td>\n",
              "      <td>0.149201</td>\n",
              "      <td>0.653682</td>\n",
              "      <td>-0.242065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6692</th>\n",
              "      <td>0.201971</td>\n",
              "      <td>1.103201</td>\n",
              "      <td>0.077293</td>\n",
              "      <td>0.035368</td>\n",
              "      <td>0.498226</td>\n",
              "      <td>0.150282</td>\n",
              "      <td>0.151656</td>\n",
              "      <td>0.311190</td>\n",
              "      <td>0.779649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20391</th>\n",
              "      <td>0.229658</td>\n",
              "      <td>1.995146</td>\n",
              "      <td>0.043844</td>\n",
              "      <td>0.003458</td>\n",
              "      <td>0.253876</td>\n",
              "      <td>0.209121</td>\n",
              "      <td>1.693763</td>\n",
              "      <td>0.145876</td>\n",
              "      <td>-0.365285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5792</th>\n",
              "      <td>0.530683</td>\n",
              "      <td>1.857228</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.010801</td>\n",
              "      <td>0.380150</td>\n",
              "      <td>0.032467</td>\n",
              "      <td>0.961106</td>\n",
              "      <td>0.301722</td>\n",
              "      <td>0.374925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37624</th>\n",
              "      <td>0.334642</td>\n",
              "      <td>1.543400</td>\n",
              "      <td>0.053172</td>\n",
              "      <td>0.009020</td>\n",
              "      <td>0.229634</td>\n",
              "      <td>0.365733</td>\n",
              "      <td>0.150038</td>\n",
              "      <td>0.485196</td>\n",
              "      <td>0.353832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40219</th>\n",
              "      <td>0.440275</td>\n",
              "      <td>1.335393</td>\n",
              "      <td>0.013469</td>\n",
              "      <td>0.000671</td>\n",
              "      <td>0.023475</td>\n",
              "      <td>0.815096</td>\n",
              "      <td>1.518595</td>\n",
              "      <td>0.469386</td>\n",
              "      <td>-0.036901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32313</th>\n",
              "      <td>0.989600</td>\n",
              "      <td>0.129331</td>\n",
              "      <td>0.023516</td>\n",
              "      <td>0.005730</td>\n",
              "      <td>0.021761</td>\n",
              "      <td>1.122677</td>\n",
              "      <td>0.069091</td>\n",
              "      <td>0.699078</td>\n",
              "      <td>-0.667638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39841</th>\n",
              "      <td>0.877417</td>\n",
              "      <td>0.062786</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>0.048548</td>\n",
              "      <td>0.248605</td>\n",
              "      <td>1.537453</td>\n",
              "      <td>1.610150</td>\n",
              "      <td>0.870066</td>\n",
              "      <td>-0.259337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17795</th>\n",
              "      <td>0.701256</td>\n",
              "      <td>0.100493</td>\n",
              "      <td>0.058984</td>\n",
              "      <td>0.039813</td>\n",
              "      <td>0.035690</td>\n",
              "      <td>1.079102</td>\n",
              "      <td>1.126777</td>\n",
              "      <td>0.924938</td>\n",
              "      <td>-0.839169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22245</th>\n",
              "      <td>0.261364</td>\n",
              "      <td>0.250220</td>\n",
              "      <td>0.007462</td>\n",
              "      <td>0.047443</td>\n",
              "      <td>0.013714</td>\n",
              "      <td>0.342149</td>\n",
              "      <td>1.156354</td>\n",
              "      <td>0.451590</td>\n",
              "      <td>0.423300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28648</th>\n",
              "      <td>0.874592</td>\n",
              "      <td>0.008672</td>\n",
              "      <td>0.010865</td>\n",
              "      <td>0.021222</td>\n",
              "      <td>0.455552</td>\n",
              "      <td>0.694042</td>\n",
              "      <td>0.311915</td>\n",
              "      <td>0.037159</td>\n",
              "      <td>0.683308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28416</th>\n",
              "      <td>0.391426</td>\n",
              "      <td>1.638694</td>\n",
              "      <td>0.047504</td>\n",
              "      <td>0.024609</td>\n",
              "      <td>0.467344</td>\n",
              "      <td>0.141297</td>\n",
              "      <td>1.709419</td>\n",
              "      <td>0.734972</td>\n",
              "      <td>0.466201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49556</th>\n",
              "      <td>0.224927</td>\n",
              "      <td>1.210898</td>\n",
              "      <td>0.051646</td>\n",
              "      <td>0.010157</td>\n",
              "      <td>0.265490</td>\n",
              "      <td>0.371841</td>\n",
              "      <td>0.166089</td>\n",
              "      <td>0.350353</td>\n",
              "      <td>0.512853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2637</th>\n",
              "      <td>0.205464</td>\n",
              "      <td>1.335393</td>\n",
              "      <td>0.001645</td>\n",
              "      <td>0.049218</td>\n",
              "      <td>0.272231</td>\n",
              "      <td>0.621764</td>\n",
              "      <td>1.705245</td>\n",
              "      <td>0.575100</td>\n",
              "      <td>-0.553990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36916</th>\n",
              "      <td>0.338553</td>\n",
              "      <td>1.914491</td>\n",
              "      <td>0.058058</td>\n",
              "      <td>0.046857</td>\n",
              "      <td>0.399522</td>\n",
              "      <td>0.381267</td>\n",
              "      <td>0.260197</td>\n",
              "      <td>0.822677</td>\n",
              "      <td>-0.581013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10448</th>\n",
              "      <td>0.578737</td>\n",
              "      <td>1.502653</td>\n",
              "      <td>0.045046</td>\n",
              "      <td>0.000284</td>\n",
              "      <td>0.102967</td>\n",
              "      <td>0.340299</td>\n",
              "      <td>0.027642</td>\n",
              "      <td>0.168833</td>\n",
              "      <td>-0.499132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46717</th>\n",
              "      <td>0.802334</td>\n",
              "      <td>1.578650</td>\n",
              "      <td>0.072476</td>\n",
              "      <td>0.019320</td>\n",
              "      <td>0.044812</td>\n",
              "      <td>0.031356</td>\n",
              "      <td>1.982347</td>\n",
              "      <td>0.154012</td>\n",
              "      <td>-0.178022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3756</th>\n",
              "      <td>0.201971</td>\n",
              "      <td>1.163303</td>\n",
              "      <td>0.012286</td>\n",
              "      <td>0.019552</td>\n",
              "      <td>0.137493</td>\n",
              "      <td>0.382032</td>\n",
              "      <td>0.676236</td>\n",
              "      <td>0.489580</td>\n",
              "      <td>0.502518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22927</th>\n",
              "      <td>0.989600</td>\n",
              "      <td>1.313777</td>\n",
              "      <td>0.079012</td>\n",
              "      <td>0.037436</td>\n",
              "      <td>0.109660</td>\n",
              "      <td>0.572237</td>\n",
              "      <td>0.372417</td>\n",
              "      <td>0.208611</td>\n",
              "      <td>-0.861987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32263</th>\n",
              "      <td>0.373799</td>\n",
              "      <td>1.409662</td>\n",
              "      <td>0.023764</td>\n",
              "      <td>0.026877</td>\n",
              "      <td>0.023927</td>\n",
              "      <td>0.830039</td>\n",
              "      <td>0.191213</td>\n",
              "      <td>0.724376</td>\n",
              "      <td>-0.596645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13739</th>\n",
              "      <td>0.590916</td>\n",
              "      <td>0.846197</td>\n",
              "      <td>0.053506</td>\n",
              "      <td>0.009998</td>\n",
              "      <td>0.481487</td>\n",
              "      <td>0.046850</td>\n",
              "      <td>1.585167</td>\n",
              "      <td>0.812196</td>\n",
              "      <td>-0.869474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8470</th>\n",
              "      <td>0.391528</td>\n",
              "      <td>1.947944</td>\n",
              "      <td>0.017481</td>\n",
              "      <td>0.006486</td>\n",
              "      <td>0.209309</td>\n",
              "      <td>0.667397</td>\n",
              "      <td>0.065715</td>\n",
              "      <td>0.876935</td>\n",
              "      <td>-0.112921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18265</th>\n",
              "      <td>0.277570</td>\n",
              "      <td>0.489535</td>\n",
              "      <td>0.067724</td>\n",
              "      <td>0.016751</td>\n",
              "      <td>0.493453</td>\n",
              "      <td>1.108177</td>\n",
              "      <td>0.523977</td>\n",
              "      <td>0.058664</td>\n",
              "      <td>-0.881303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44073</th>\n",
              "      <td>0.810456</td>\n",
              "      <td>1.613509</td>\n",
              "      <td>0.079026</td>\n",
              "      <td>0.047443</td>\n",
              "      <td>0.458519</td>\n",
              "      <td>1.985412</td>\n",
              "      <td>0.703424</td>\n",
              "      <td>0.219107</td>\n",
              "      <td>0.222075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19267</th>\n",
              "      <td>0.902621</td>\n",
              "      <td>0.172176</td>\n",
              "      <td>0.018784</td>\n",
              "      <td>0.041871</td>\n",
              "      <td>0.064603</td>\n",
              "      <td>1.685501</td>\n",
              "      <td>0.375954</td>\n",
              "      <td>0.770560</td>\n",
              "      <td>-0.469839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28797</th>\n",
              "      <td>0.305722</td>\n",
              "      <td>1.672154</td>\n",
              "      <td>0.043157</td>\n",
              "      <td>0.021138</td>\n",
              "      <td>0.263004</td>\n",
              "      <td>1.586433</td>\n",
              "      <td>0.254102</td>\n",
              "      <td>0.958788</td>\n",
              "      <td>0.213872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14561</th>\n",
              "      <td>0.703561</td>\n",
              "      <td>0.072972</td>\n",
              "      <td>0.068550</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.248605</td>\n",
              "      <td>1.532844</td>\n",
              "      <td>1.846427</td>\n",
              "      <td>0.271077</td>\n",
              "      <td>0.867225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42161</th>\n",
              "      <td>0.360970</td>\n",
              "      <td>0.648508</td>\n",
              "      <td>0.013586</td>\n",
              "      <td>0.041877</td>\n",
              "      <td>0.028298</td>\n",
              "      <td>0.833892</td>\n",
              "      <td>0.572515</td>\n",
              "      <td>0.537903</td>\n",
              "      <td>-0.875486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38359</th>\n",
              "      <td>0.338553</td>\n",
              "      <td>0.228158</td>\n",
              "      <td>0.025037</td>\n",
              "      <td>0.029819</td>\n",
              "      <td>0.069665</td>\n",
              "      <td>0.350182</td>\n",
              "      <td>0.558028</td>\n",
              "      <td>0.954863</td>\n",
              "      <td>0.459732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29663</th>\n",
              "      <td>0.261364</td>\n",
              "      <td>1.073403</td>\n",
              "      <td>0.060569</td>\n",
              "      <td>0.013604</td>\n",
              "      <td>0.283790</td>\n",
              "      <td>0.250544</td>\n",
              "      <td>1.759530</td>\n",
              "      <td>0.183378</td>\n",
              "      <td>-0.893004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10023</th>\n",
              "      <td>0.205464</td>\n",
              "      <td>1.313681</td>\n",
              "      <td>0.013451</td>\n",
              "      <td>0.002058</td>\n",
              "      <td>0.493146</td>\n",
              "      <td>0.602416</td>\n",
              "      <td>1.822098</td>\n",
              "      <td>0.079310</td>\n",
              "      <td>-0.366569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39949</th>\n",
              "      <td>0.232278</td>\n",
              "      <td>0.055581</td>\n",
              "      <td>0.026690</td>\n",
              "      <td>0.047569</td>\n",
              "      <td>0.432851</td>\n",
              "      <td>1.010763</td>\n",
              "      <td>1.529575</td>\n",
              "      <td>0.290168</td>\n",
              "      <td>-0.293424</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2fb27c5-632a-4b41-b545-60483e8cccef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e2fb27c5-632a-4b41-b545-60483e8cccef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e2fb27c5-632a-4b41-b545-60483e8cccef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([y_test_xgb[ind_itm],predict_[ind_itm]], axis=1).loc[(y_test_xgb[ind_itm] - predict_[ind_itm]).abs().sort_values(ascending=False).head(50).index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KI8N2vKb_CkF",
        "outputId": "6df6daac-48f6-4ed3-d894-5aae3392e293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       eurocall_fourier         0\n",
              "14677          0.000000  0.701283\n",
              "9870           0.000000  0.149616\n",
              "45809          0.462180  0.435609\n",
              "2333           0.132113  0.151808\n",
              "39685          0.469654  0.487611\n",
              "22626          0.798086  0.815389\n",
              "10746          0.154220  0.136956\n",
              "39375          0.325183  0.308076\n",
              "32209          0.116622  0.133631\n",
              "16589          0.748105  0.731518\n",
              "43384          0.058210  0.074701\n",
              "38188          0.199523  0.215676\n",
              "20297          0.370807  0.386557\n",
              "2353           0.327448  0.342900\n",
              "41986          0.027783  0.042566\n",
              "43911          0.725219  0.710881\n",
              "4421           0.470495  0.456174\n",
              "6864           0.161095  0.175221\n",
              "31306          0.430561  0.416595\n",
              "6692           0.776479  0.762549\n",
              "20391          0.783973  0.770472\n",
              "5792           0.478781  0.492257\n",
              "37624          0.681042  0.667571\n",
              "40219          0.604481  0.617853\n",
              "32313          0.029301  0.042647\n",
              "39841          0.132235  0.145489\n",
              "17795          0.299045  0.312088\n",
              "22245          0.727363  0.714485\n",
              "28648          0.125597  0.138317\n",
              "28416          0.604555  0.617204\n",
              "49556          0.776585  0.764108\n",
              "2637           0.737482  0.725044\n",
              "36916          0.640681  0.653053\n",
              "10448          0.470535  0.458255\n",
              "46717          0.264497  0.252360\n",
              "3756           0.778450  0.766459\n",
              "22927          0.217467  0.205518\n",
              "32263          0.607240  0.595312\n",
              "13739          0.460246  0.448332\n",
              "8470           0.629891  0.618176\n",
              "18265          0.723977  0.712359\n",
              "44073          0.514937  0.526499\n",
              "19267          0.113528  0.124766\n",
              "28797          0.699231  0.710438\n",
              "14561          0.300091  0.311241\n",
              "42161          0.616386  0.627518\n",
              "38359          0.656611  0.645496\n",
              "29663          0.741426  0.730344\n",
              "10023          0.800017  0.788958\n",
              "39949          0.765459  0.754524"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-76de1f9a-4cdb-40e1-9e92-1d63f1f073a3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eurocall_fourier</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14677</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.701283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9870</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.149616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45809</th>\n",
              "      <td>0.462180</td>\n",
              "      <td>0.435609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333</th>\n",
              "      <td>0.132113</td>\n",
              "      <td>0.151808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39685</th>\n",
              "      <td>0.469654</td>\n",
              "      <td>0.487611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22626</th>\n",
              "      <td>0.798086</td>\n",
              "      <td>0.815389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10746</th>\n",
              "      <td>0.154220</td>\n",
              "      <td>0.136956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39375</th>\n",
              "      <td>0.325183</td>\n",
              "      <td>0.308076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32209</th>\n",
              "      <td>0.116622</td>\n",
              "      <td>0.133631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16589</th>\n",
              "      <td>0.748105</td>\n",
              "      <td>0.731518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43384</th>\n",
              "      <td>0.058210</td>\n",
              "      <td>0.074701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38188</th>\n",
              "      <td>0.199523</td>\n",
              "      <td>0.215676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20297</th>\n",
              "      <td>0.370807</td>\n",
              "      <td>0.386557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2353</th>\n",
              "      <td>0.327448</td>\n",
              "      <td>0.342900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41986</th>\n",
              "      <td>0.027783</td>\n",
              "      <td>0.042566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43911</th>\n",
              "      <td>0.725219</td>\n",
              "      <td>0.710881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4421</th>\n",
              "      <td>0.470495</td>\n",
              "      <td>0.456174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6864</th>\n",
              "      <td>0.161095</td>\n",
              "      <td>0.175221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31306</th>\n",
              "      <td>0.430561</td>\n",
              "      <td>0.416595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6692</th>\n",
              "      <td>0.776479</td>\n",
              "      <td>0.762549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20391</th>\n",
              "      <td>0.783973</td>\n",
              "      <td>0.770472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5792</th>\n",
              "      <td>0.478781</td>\n",
              "      <td>0.492257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37624</th>\n",
              "      <td>0.681042</td>\n",
              "      <td>0.667571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40219</th>\n",
              "      <td>0.604481</td>\n",
              "      <td>0.617853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32313</th>\n",
              "      <td>0.029301</td>\n",
              "      <td>0.042647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39841</th>\n",
              "      <td>0.132235</td>\n",
              "      <td>0.145489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17795</th>\n",
              "      <td>0.299045</td>\n",
              "      <td>0.312088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22245</th>\n",
              "      <td>0.727363</td>\n",
              "      <td>0.714485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28648</th>\n",
              "      <td>0.125597</td>\n",
              "      <td>0.138317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28416</th>\n",
              "      <td>0.604555</td>\n",
              "      <td>0.617204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49556</th>\n",
              "      <td>0.776585</td>\n",
              "      <td>0.764108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2637</th>\n",
              "      <td>0.737482</td>\n",
              "      <td>0.725044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36916</th>\n",
              "      <td>0.640681</td>\n",
              "      <td>0.653053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10448</th>\n",
              "      <td>0.470535</td>\n",
              "      <td>0.458255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46717</th>\n",
              "      <td>0.264497</td>\n",
              "      <td>0.252360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3756</th>\n",
              "      <td>0.778450</td>\n",
              "      <td>0.766459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22927</th>\n",
              "      <td>0.217467</td>\n",
              "      <td>0.205518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32263</th>\n",
              "      <td>0.607240</td>\n",
              "      <td>0.595312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13739</th>\n",
              "      <td>0.460246</td>\n",
              "      <td>0.448332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8470</th>\n",
              "      <td>0.629891</td>\n",
              "      <td>0.618176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18265</th>\n",
              "      <td>0.723977</td>\n",
              "      <td>0.712359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44073</th>\n",
              "      <td>0.514937</td>\n",
              "      <td>0.526499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19267</th>\n",
              "      <td>0.113528</td>\n",
              "      <td>0.124766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28797</th>\n",
              "      <td>0.699231</td>\n",
              "      <td>0.710438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14561</th>\n",
              "      <td>0.300091</td>\n",
              "      <td>0.311241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42161</th>\n",
              "      <td>0.616386</td>\n",
              "      <td>0.627518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38359</th>\n",
              "      <td>0.656611</td>\n",
              "      <td>0.645496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29663</th>\n",
              "      <td>0.741426</td>\n",
              "      <td>0.730344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10023</th>\n",
              "      <td>0.800017</td>\n",
              "      <td>0.788958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39949</th>\n",
              "      <td>0.765459</td>\n",
              "      <td>0.754524</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76de1f9a-4cdb-40e1-9e92-1d63f1f073a3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-76de1f9a-4cdb-40e1-9e92-1d63f1f073a3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-76de1f9a-4cdb-40e1-9e92-1d63f1f073a3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(y_test_xgb - predict_).abs().sort_values(ascending=False).head(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmZtp8Rp-LWx",
        "outputId": "bb923d99-9cdd-4035-f160-446cdfd2e2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14677    0.701283\n",
              "9870     0.149616\n",
              "46558    0.037334\n",
              "42867    0.030292\n",
              "47247    0.029382\n",
              "35373    0.027949\n",
              "45809    0.026571\n",
              "43269    0.024211\n",
              "31932    0.023699\n",
              "19130    0.020664\n",
              "29442    0.020481\n",
              "2333     0.019696\n",
              "39685    0.017957\n",
              "22626    0.017303\n",
              "10746    0.017264\n",
              "39375    0.017106\n",
              "32209    0.017009\n",
              "16589    0.016588\n",
              "43384    0.016490\n",
              "20470    0.016278\n",
              "38188    0.016153\n",
              "20297    0.015750\n",
              "2353     0.015452\n",
              "17337    0.015322\n",
              "13956    0.015248\n",
              "10232    0.015211\n",
              "41986    0.014783\n",
              "9907     0.014713\n",
              "38905    0.014547\n",
              "43911    0.014337\n",
              "4421     0.014321\n",
              "38037    0.014300\n",
              "6864     0.014125\n",
              "31306    0.013966\n",
              "6692     0.013931\n",
              "29091    0.013844\n",
              "13854    0.013842\n",
              "48153    0.013723\n",
              "38623    0.013618\n",
              "20391    0.013501\n",
              "5792     0.013476\n",
              "37624    0.013472\n",
              "40219    0.013372\n",
              "43249    0.013347\n",
              "32313    0.013346\n",
              "10415    0.013336\n",
              "39841    0.013254\n",
              "26917    0.013141\n",
              "17795    0.013043\n",
              "38215    0.013025\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(y_test_xgb[ind_tm]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sJj4P0U90EJ",
        "outputId": "a0bc7b41-ae6c-4db0-c3c8-08d309d0c3ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2724,)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ind_otm = X_test_xgb.m > 1\n",
        "print((y_test_xgb[ind_otm] - predict_[ind_otm]).abs().quantile([0,0.25,0.5,0.75,0.85,0.9,0.95, 0.99,1]).round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbX039I39n7f",
        "outputId": "5de061c5-6b7c-4b91-c1ea-f061d25d5283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00    0.000\n",
            "0.25    0.000\n",
            "0.50    0.001\n",
            "0.75    0.002\n",
            "0.85    0.003\n",
            "0.90    0.003\n",
            "0.95    0.004\n",
            "0.99    0.007\n",
            "1.00    0.037\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('ANN Absolute Error (quantile): ')\n",
        "test_dataset = torch.utils.data.DataLoader(PricingDataset(X_test_xgb, y_test_xgb), batch_size=512)\n",
        "with torch.no_grad():\n",
        "   print((y_test_xgb - pd.Series(model(torch.tensor(X_test_xgb.values.astype(np.float32))).flatten(), index=X_test_xgb.index)).abs().quantile([0,0.25,0.5,0.75,0.85,0.9,0.95, 0.99,1]).round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFTWfMW3uDFy",
        "outputId": "e2d28477-4c53-4e7e-fe66-0ed972d75b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN Absolute Error (quantile): \n",
            "0.00    0.000\n",
            "0.25    0.000\n",
            "0.50    0.001\n",
            "0.75    0.002\n",
            "0.85    0.003\n",
            "0.90    0.004\n",
            "0.95    0.006\n",
            "0.99    0.009\n",
            "1.00    0.701\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(epoch):\n",
        "  train_dataset = torch.utils.data.DataLoader(PricingDataset(X_train_xgb, y_train_xgb), batch_size=128)\n",
        "  val_dataset = torch.utils.data.DataLoader(PricingDataset(X_val_xgb, y_val_xgb), batch_size=128)\n",
        "  for t in range(epochs):\n",
        "      train_loop(train_dataset, model, loss_fn, optimizer)\n",
        "      val_loss = test_loop(val_dataset, model, loss_fn)\n",
        "      scheduler.step(val_loss)\n",
        "      if t > 25:\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        print(f\"Avg loss: {val_loss:>8f} \\n\")\n",
        "  print(\"Done!\")"
      ],
      "metadata": {
        "id": "XsM6-R9MPM3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, '/content/drive/MyDrive/practicum_data-American_Option/euro_transform.pt')"
      ],
      "metadata": {
        "id": "lplbP7pjSelg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mm = torch.load('/content/drive/MyDrive/practicum_data-American_Option/without_eurocall.pt')\n",
        "print('ANN Absolute Error (quantile): ')\n",
        "with torch.no_grad():\n",
        "   print((y_test_xgb - pd.Series(mm(torch.tensor(X_test_xgb.values.astype(np.float32))).flatten(), index=X_test_xgb.index)).abs().quantile([0,0.25,0.5,0.75,0.85,0.9,0.95, 0.99,1]).round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2Ok173ypUZZ",
        "outputId": "dedbd1c0-d1ef-4421-fcb0-731f5583975e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN Absolute Error (quantile): \n",
            "0.00    0.000\n",
            "0.25    0.000\n",
            "0.50    0.001\n",
            "0.75    0.003\n",
            "0.85    0.004\n",
            "0.90    0.005\n",
            "0.95    0.007\n",
            "0.99    0.012\n",
            "1.00    0.048\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_ = X_test_xgb.sort_values(by=\"v0\")\n",
        "y_ = y_test_xgb[x_.index]\n",
        "plt.plot(x_.v0, y_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "J1sAf2LsSEC2",
        "outputId": "78f5d51f-6e2f-406c-f4c5-d5f64dedb40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8311b66e80>]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch(35)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65gTSYJvPae5",
        "outputId": "6363db4f-0285-487f-c646-7181c95fbb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27\n",
            "-------------------------------\n",
            "Avg loss: 0.000004 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Avg loss: 0.000004 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Avg loss: 0.000003 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Avg loss: 0.000006 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Avg loss: 0.000004 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Avg loss: 0.000003 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Avg loss: 0.000003 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Avg loss: 0.000003 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Avg loss: 0.000003 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Avg loss: 0.000003 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Avg loss: 0.000003 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Avg loss: 0.000002 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Avg loss: 0.000003 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Avg loss: 0.000005 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Avg loss: 0.000003 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Avg loss: 0.000007 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Avg loss: 0.000004 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Avg loss: 0.000003 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Avg loss: 0.000002 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Avg loss: 0.000003 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Avg loss: 0.000002 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Avg loss: 0.000002 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Avg loss: 0.000002 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Avg loss: 0.000002 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_xgb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcqJK28ePrgn",
        "outputId": "f4428c87-dbe1-424b-b66b-1cfe28cc1a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uaPEVeGVsJXB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}